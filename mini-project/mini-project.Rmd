---
title: 'Class 08: Mini-project'
author: "Ramola Baviskar (PID A12228297)"
date: "2/10/2022"
output:
  pdf_document: default
  html_document: default
---
#read.csv("yourfilename")
#prcomp(x, scale=TRUE)
#kmeans(x, centers=?)
#hclust(dist(x))

Unsupervised Learning Analysis of Human Breast Cancer Cells
Here is some data from U of Wisconsin Medical Center.
```{r}

wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)

```
How many rows (ie patients) & columns?
```{r}
nrow(wisc.df)
ncol(wisc.df)
```
>Q1. How many observations are in this dataset?
There are 569 observations.

>Q2. How many of the observations have a malignant diagnosis?
```{r}
sum(wisc.df$diagnosis == "M")



```
A useful function that we'll use is the table function.
```{r}
table(wisc.df$diagnosis)
```
Q3. How many variables/features in the data are suffixed with _mean?
```{r}

colnames(wisc.df)
 grep("_mean", colnames(wisc.df))
 length(grep("_mean", colnames(wisc.df)))
```


Let's remove the first column because that's the expert diagnosis (essentially the answer).
```{r}
wisc.data <- wisc.df[,-1]
diagnosis <- as.factor(wisc.df$diagnosis)

```
#PCA
We need to scale the data before PCA as the various variables.
```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```
>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
  0.4427
>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
  3
>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
  7

My result: the "PCA plot" aka "score plot" or "PC1 v. PC2 plot"
```{r}
```
>Q7: It is not easy to understand.

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
  They're more clear and there is definitely a difference between malignant/benign diagnoses. The PC1 & 2 plot shows a more clear distinction than the PC1 & 3 plot. PC1 actually appears to be the more reliable indicator of B/M.
```{r}
plot(wisc.pr$x[, c(1, 3)], col=diagnosis)
```





```{r}
library(ggplot2)
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```


```{r}
pr.var <- wisc.pr$sdev^2
pve <- pr.var/sum(pr.var)
pve

wisc.pr$rotation[,1]

```
```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```



>>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
    -0.26085376

>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
  5


##Hierarchical Clustering
```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
```


```{r}
wisc.hclust <- hclust(data.dist)
```
>>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
 19
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```



Let's try to cluster the raw data.

```{r}
hc <- hclust(dist(wisc.data))
plot(hc)
wisc.hclust.clusters <- cutree(hc, k=4, h=NULL)

```



```{r}
table(wisc.hclust.clusters, diagnosis)
```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.clusters <- cutree(hc, k=5, h=NULL)
table(wisc.hclust.clusters, diagnosis)
```

We can combine methods to be useful. We can take the PCA results and apply clustering to them.
```{r}

wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)

```

```{r}
plot(wisc.pr$x[,1:2], col=grps)

```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```




>Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
```
>Q16.
```{r}

table(wisc.hclust.clusters, diagnosis)
```

>Q17. hierarchical clusters; PCA



#Predictions
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)

npc
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

>Q18. Which of these new patients should we prioritize for follow up based on your results?
  The black group (labeled 2)















